# pytorch-learning
A collection of practice notebooks for mastering PyTorch, covering activation functions, softmax, shallow neural networks, autograd, and linear regression.
# PyTorch Practice Projects

This repository contains a collection of Jupyter notebooks created to strengthen my understanding of PyTorch and core deep learning concepts. The notebooks cover a variety of topics including activation functions, shallow neural networks, regression, softmax, and autograd.

## üìÅ Contents

| Notebook | Description |
|----------|-------------|
| `Activation_Functions.ipynb` | Visual exploration of ReLU, Sigmoid, and Tanh functions |
| `Shallow_Neural_Networks.ipynb` | Implementation and training of basic neural networks from scratch |
| `Softmax_practice.ipynb` | Manual implementation of the softmax function and its behavior |
| `Softmax_in_Pytorch.ipynb` | Use of PyTorch‚Äôs built-in softmax layer for classification tasks |
| `Regression_using_Pytorch.ipynb` | Simple linear regression using PyTorch |
| `Multiple_Linear_Regression_Prediction.ipynb` | Linear regression with multiple input features |
| `customautograd.ipynb` | Example of building a custom autograd function for backpropagation |
| `PyTorch_Practice.ipynb` | General model training practice using PyTorch |

## üöÄ Getting Started

To run these notebooks on your local machine, follow the steps below.

### Option 1: Manual Installation

Install the required Python libraries:

```bash
pip install torch matplotlib notebook


### Option 2: Install All at Once with requirements.txt

If you prefer, use the pre-defined dependencies:
pip install -r requirements.txt


### Run the Notebooks

After installing dependencies, launch Jupyter:
jupyter notebook

yaml

## üß† What I Learned

- Core operations in PyTorch (tensors, autograd, optimizers)
- Building and training neural networks manually and with nn.Module
- Visualizing activation functions and model predictions
- Writing custom backward passes with autograd.Function

## üìÑ License

This project is licensed under the MIT License.

---

Author: Ottikunta Sahith  
Master‚Äôs Student in Computer Science @ Stevens Institute of Technology

